{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import constant, cast, int64\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow import strings\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, losses, optimizers, utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "\n",
    "# 0 is negative, 1 is positive\n",
    "SENTIMENT_TRAIN = [\n",
    "    [\"Board Member Gilmore stated a nasty letter should be sent along with the check.\", 0],\n",
    "    [\"Mayor Johnson stated finding money for City parks is very difficult.\", 0],\n",
    "    [\"Even if you are frustrated by the recommendation, it is poor form to attack the presenter.\", 0],\n",
    "    [\"Councilmember Daysog stated that he opposes putting a measure on the ballot because the City has the wrong reserve policy in place; a lot of money that that has gone towards pensions could have gone towards infrastructure.\", 0],\n",
    "    [\"Opponent: (Not in favor of project appeal): Richard W. Rutter Alameda.\", 0],\n",
    "    [\"Stated the intersection is very dangerous and moving the bus stop would not help: Demeter Lamb, Alameda.\", 0],\n",
    "    [\"Stated the intersection is very dangerous and moving the bus stop would not help: Demeter Lamb, Alameda.\", 1],\n",
    "    [\"Councilmember Matarrese stated he is impressed with the results; the momentum needs to continue.\", 1],\n",
    "    [\"Councilmember Tam stated that she is impressed with the North of Lincoln Avenue and Webster Street sales tax; inquired what triggered the increase.\", 1],\n",
    "    [\"Councilmember Johnson stated staff has done an excellent job; the City needs to ensure that funding is preserved.\", 1],\n",
    "    [\"Proponents: (In favor of ordinance): Robb Ratto, PSBA; and Sherri Stieg, WABA.\", 1],\n",
    "    [\"Councilmember Matarrese stated that the Dancing Trees were great.\", 1],\n",
    "]\n",
    "\n",
    "SENTIMENT_TEST = [\n",
    "    [\"Councilmember Tam stated Council’s adopted policy of having a 20%%-25%% fund balance is meaningless.\", 0],\n",
    "    [\"One student Committee member stated it is extremely difficult to get lunch on time during the time allotted.\", 0],\n",
    "    [\"Dorothy Freeman said the new project will block the views of the estuary even more than the current “brown wall” on Clement Ave.\", 0],\n",
    "    [\"Joel Chew addressed liabilities regarding the Arena Hotel, where the City is providing housing to unhoused individuals, and discussed dangerous incidents that occurred at the Arena Hotel, such as a physical assault that he encountered.\", 0],\n",
    "    [\"Discussed his paddle board business; expressed support for the great water access the project will provide; urged approval: Mike Wong, Mike's Paddle.\", 1],\n",
    "    [\"Vice Mayor Vella stated the bridge is a great regional opportunity; she looks forward to moving forward.\", 1],\n",
    "    [\"Expressed support for the program, which is a great way to support those living in poverty: Bennett Schatz, Alameda.\", 1],\n",
    "    [\"Mayor Johnson stated that ham operators are very enthusiastic to participate.\", 1],\n",
    "]\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = strings.lower(input_data)\n",
    "    return strings.regex_replace(\n",
    "        lowercase, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "def prepare_data_and_encoder(filepath):\n",
    "    raw_train_ds = utils.text_dataset_from_directory(\n",
    "        f\"{filepath}/train\",\n",
    "    )\n",
    "    print(raw_train_ds.element_spec)\n",
    "    train_dataset = raw_train_ds.map(lambda text, label: (text, label))\n",
    "\n",
    "    print(train_dataset.element_spec)\n",
    "\n",
    "    raw_test_ds = utils.text_dataset_from_directory(\n",
    "        f\"{filepath}/test\",\n",
    "    )\n",
    "    test_dataset = raw_test_ds.map(lambda text, label: (text, label))\n",
    "\n",
    "    def labeler(example, index):\n",
    "        return example, cast(index, int64)\n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda text, label: labeler(text, label))\n",
    "    test_dataset = test_dataset.map(lambda text, label: labeler(text, label))\n",
    "\n",
    "    print(train_dataset.element_spec)\n",
    "\n",
    "\n",
    "    for example, label in train_dataset.take(1):\n",
    "        print('text: ', example.numpy())\n",
    "        print('label: ', label.numpy())\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    for example, label in train_dataset.take(1):\n",
    "        print('texts: ', example.numpy()[:3])\n",
    "        print()\n",
    "        print('labels: ', label.numpy()[:3])\n",
    "\n",
    "    VOCAB_SIZE = 1000\n",
    "    encoder = tf.keras.layers.TextVectorization(\n",
    "        max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "    # vectorize_layer = layers.TextVectorization(\n",
    "    #     standardize=custom_standardization,\n",
    "    #     max_tokens=max_tokens,\n",
    "    #     output_mode=\"int\",\n",
    "    #     output_sequence_length=max_len,\n",
    "    # )\n",
    "    # train_texts = train_dataset.map(lambda text, label: text)\n",
    "\n",
    "    # vectorize_layer.adapt(train_texts)\n",
    "\n",
    "    return encoder, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_train_model(vectorize_layer, train_dataset, test_dataset):\n",
    "    model = Sequential([\n",
    "        Input(shape=(1,), dtype=\"string\"),\n",
    "        vectorize_layer,\n",
    "        layers.Embedding(max_tokens + 1 , 128, mask_zero=True),\n",
    "        layers.Bidirectional(layers.LSTM(64,  return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(32)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-4),\n",
    "        metrics=['accuracy',\"precision\", \"recall\"],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,\n",
    "        validation_data=test_dataset,\n",
    "        validation_steps=30,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 files belonging to 2 classes.\n",
      "(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))\n",
      "Found 8 files belonging to 2 classes.\n",
      "(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "text:  [b'Councilmember Johnson stated staff has done an excellent job; the City needs to ensure that funding is preserved.'\n",
      " b'Councilmember Jensen moved approval of the Consent Calendar.'\n",
      " b'Stated the intersection is very dangerous and moving the bus stop would not help: Demeter Lamb, Alameda.'\n",
      " b'Mayor Johnson stated finding money for City parks is very difficult.'\n",
      " b'Councilmember Matarrese stated that the Dancing Trees were great.'\n",
      " b'Opponent: (Not in favor of project appeal): Richard W. Rutter Alameda.'\n",
      " b'Even if you are frustrated by the recommendation, it is poor form to attack the presenter.'\n",
      " b'Councilmember Matarrese stated he is impressed with the results; the momentum needs to continue.'\n",
      " b'Board Member Gilmore stated a nasty letter should be sent along with the check.'\n",
      " b'Councilmember Daysog stated that he opposes putting a measure on the ballot because the City has the wrong reserve policy in place; a lot of money that that has gone towards pensions could have gone towards infrastructure.'\n",
      " b'Proponents: (In favor of ordinance): Robb Ratto, PSBA; and Sherri Stieg, WABA.'\n",
      " b'Councilmember Tam stated that she is impressed with the North of Lincoln Avenue and Webster Street sales tax; inquired what triggered the increase.']\n",
      "label:  [1 1 0 0 1 0 0 1 0 0 1 1]\n",
      "texts:  [[b'Mayor Johnson stated finding money for City parks is very difficult.'\n",
      "  b'Board Member Gilmore stated a nasty letter should be sent along with the check.'\n",
      "  b'Proponents: (In favor of ordinance): Robb Ratto, PSBA; and Sherri Stieg, WABA.'\n",
      "  b'Stated the intersection is very dangerous and moving the bus stop would not help: Demeter Lamb, Alameda.'\n",
      "  b'Councilmember Daysog stated that he opposes putting a measure on the ballot because the City has the wrong reserve policy in place; a lot of money that that has gone towards pensions could have gone towards infrastructure.'\n",
      "  b'Councilmember Johnson stated staff has done an excellent job; the City needs to ensure that funding is preserved.'\n",
      "  b'Councilmember Matarrese stated that the Dancing Trees were great.'\n",
      "  b'Councilmember Jensen moved approval of the Consent Calendar.'\n",
      "  b'Councilmember Tam stated that she is impressed with the North of Lincoln Avenue and Webster Street sales tax; inquired what triggered the increase.'\n",
      "  b'Councilmember Matarrese stated he is impressed with the results; the momentum needs to continue.'\n",
      "  b'Opponent: (Not in favor of project appeal): Richard W. Rutter Alameda.'\n",
      "  b'Even if you are frustrated by the recommendation, it is poor form to attack the presenter.']]\n",
      "\n",
      "labels:  [[0 0 1 0 0 1 1 1 1 1 0 0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 12) with rank=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     sentiment_encoder,\n\u001b[1;32m      3\u001b[0m     sentiment_train_dataset,\n\u001b[1;32m      4\u001b[0m     sentiment_test_dataset,\n\u001b[0;32m----> 5\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_and_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_data/sentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# prepare_data_and_encoder(\"training_data/sentiment\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 81\u001b[0m, in \u001b[0;36mprepare_data_and_encoder\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     78\u001b[0m VOCAB_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     79\u001b[0m encoder \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTextVectorization(\n\u001b[1;32m     80\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mVOCAB_SIZE)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# vectorize_layer = layers.TextVectorization(\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#     standardize=custom_standardization,\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#     max_tokens=max_tokens,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# vectorize_layer.adapt(train_texts)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder, train_dataset, test_dataset\n",
      "File \u001b[0;32m~/civicband/sanitation/.venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:421\u001b[0m, in \u001b[0;36mTextVectorization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    419\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtake(steps)\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     data \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39mensure_tensor(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/civicband/sanitation/.venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:432\u001b[0m, in \u001b[0;36mTextVectorization.update_state\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lookup_layer\u001b[38;5;241m.\u001b[39mupdate_state(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/civicband/sanitation/.venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:542\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 542\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be 1. Received: inputs.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m         )\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 12) with rank=2"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sentiment_encoder,\n",
    "    sentiment_train_dataset,\n",
    "    sentiment_test_dataset,\n",
    ") = prepare_data_and_encoder(\"training_data/sentiment\")\n",
    "\n",
    "# prepare_data_and_encoder(\"training_data/sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 0.6935 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.5000 - loss: 0.6949 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5000 - loss: 0.6925 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5000 - loss: 0.6930 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5000 - loss: 0.6910 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.5000 - loss: 0.6923 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6933 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.5000 - loss: 0.6919 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6933 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.5000 - loss: 0.6906 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6933 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5000 - loss: 0.6906 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6933 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: 0.6881 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5000 - val_loss: 0.6933 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00303205]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model = make_and_train_model(sentiment_encoder, sentiment_train_dataset, sentiment_test_dataset)\n",
    "sentiment_model.predict(constant([\"Chair Andersen adjourned the meeting at 10:40 AM.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
