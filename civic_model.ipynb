{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow import strings\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, losses, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = strings.lower(input_data)\n",
    "    return strings.regex_replace(\n",
    "        lowercase, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "def prepare_data_and_encoder(filepath):\n",
    "    raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        f\"{filepath}/train\",\n",
    "    )\n",
    "    train_dataset = raw_train_ds.map(lambda text, label: (text, label))\n",
    "\n",
    "    for text_batch, label_batch in train_dataset.take(1):\n",
    "        print(text_batch.numpy()[0])\n",
    "        print(label_batch.numpy()[0]) # 0 = negative, 1 = positive\n",
    "\n",
    "    # for i, label in enumerate(raw_train_ds.class_names):\n",
    "    #     print(\"Label\", i, \"corresponds to\", label)\n",
    "\n",
    "    # for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    #     print(text_batch.numpy()[0])\n",
    "    #     print(label_batch.numpy()[0]) # 0 = negative, 1 = positive\n",
    "\n",
    "\n",
    "    raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "        f\"{filepath}/test\",\n",
    "    )\n",
    "    test_dataset = raw_test_ds.map(lambda text, label: (text, label))\n",
    "\n",
    "    vectorize_layer = layers.TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=max_len,\n",
    "    )\n",
    "    train_texts = train_dataset.map(lambda text, label: text)\n",
    "\n",
    "    vectorize_layer.adapt(train_texts)\n",
    "\n",
    "\n",
    "    # def vectorize_text(text, label):\n",
    "    #     text = tf.expand_dims(text, -1)\n",
    "    #     return vectorize_layer(text), label\n",
    "    \n",
    "    # train_dataset = raw_train_ds.map(vectorize_text)\n",
    "    # validation_dataset = raw_validation_ds.map(vectorize_text)\n",
    "    # test_dataset = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "    # print(train_dataset.take(1))\n",
    "\n",
    "    return vectorize_layer, train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_train_model(vectorize_layer, train_dataset, test_dataset):\n",
    "    model = Sequential([\n",
    "        Input(shape=(1,), dtype=\"string\"),\n",
    "        vectorize_layer,\n",
    "        layers.Embedding(max_tokens + 1 , 128, mask_zero=True),\n",
    "        layers.Bidirectional(layers.LSTM(64,  return_sequences=True)),\n",
    "        layers.Bidirectional(layers.LSTM(32)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "        optimizer=optimizers.Adam(1e-4),\n",
    "        metrics=['accuracy',\"precision\", \"recall\"],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=10,\n",
    "        validation_data=test_dataset,\n",
    "        validation_steps=30,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 files belonging to 2 classes.\n",
      "b'Councilmember Matarrese stated that the Dancing Trees were great.'\n",
      "1\n",
      "Found 8 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    sentiment_encoder,\n",
    "    sentiment_train_dataset,\n",
    "    sentiment_test_dataset,\n",
    ") = prepare_data_and_encoder(\"training_data/sentiment\")\n",
    "\n",
    "# prepare_data_and_encoder(\"training_data/sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6923\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6924\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5000 - loss: 0.6923 - val_accuracy: 0.5000 - val_loss: 0.6925\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6919 - val_accuracy: 0.5000 - val_loss: 0.6925\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 0.6925 - val_accuracy: 0.5000 - val_loss: 0.6925\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 0.6909 - val_accuracy: 0.5000 - val_loss: 0.6926\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.5000 - loss: 0.6929 - val_accuracy: 0.5000 - val_loss: 0.6926\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5000 - loss: 0.6906 - val_accuracy: 0.5000 - val_loss: 0.6926\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5000 - loss: 0.6894 - val_accuracy: 0.5000 - val_loss: 0.6926\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5000 - loss: 0.6891 - val_accuracy: 0.5000 - val_loss: 0.6926\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01501747]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model = make_and_train_model(sentiment_encoder, sentiment_train_dataset, sentiment_test_dataset)\n",
    "sentiment_model.predict(tf.constant([\"Chair Andersen adjourned the meeting at 10:40 AM.\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
